{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nDifferent weights in block mean\n===============================\n\n:class:`verde.BlockReduce` is not able to output weights because we need to\nmake assumptions about the reduction operation to know how to propagate\nuncertainties or calculated weighted variances.\nThat's why verde provides specialized reductions like :class:`verde.BlockMean`,\nwhich can calculate weights from input data in three ways:\n\n1. Using the variance of the data. This is the only possible option when no\n   weights are provided.\n2. Using the uncertainty of the weighted mean propagated from the uncertainties\n   in the data. In this case, we assume that the weights are\n   ``1/uncertainty**2``.\n3. Using the weighted variance of the data.\n\nUsing the propagated uncertainties may be more adequate if your data is smooth\nin each block (low variance) but have very different uncertainties. The\npropagation preserves a low weight for data that have large uncertainties but\ndon't vary much inside the block.\n\nThe weighted variance should be used when the data vary a lot in each block\n(high variance) but have very similar uncertainties. The variance will be large\nwhen there is a lot of variability in the data that isn't due to the\nuncertainties. This is also the best choice if your data weights aren't\n``1/uncertainty**2``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom matplotlib.colors import PowerNorm, LogNorm\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\n# We need these two classes to set proper ticklabels for Cartopy maps\nfrom cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\nimport numpy as np\nimport verde as vd\n\n# We'll test this on the California vertical GPS velocity data because it comes\n# with the uncertainties\ndata = vd.datasets.fetch_california_gps()\ncoordinates = (data.longitude, data.latitude)\n\n# We'll calculate the mean on large blocks to show the effect of the different\n# weighting schemes\nspacing = 30/60\n# It's important that the weights are given as 1/sigma**2 for the uncertainty\n# propagation. In this case, you should not use verde.variance_to_weights\n# because it would normalize the weights.\nweights = 1/data.std_up**2\nreducer = vd.BlockMean(spacing, center_coordinates=True)\n# First produce the weighted variance weights\nvariance_weights = reducer.filter(coordinates, data.velocity_up, weights)[-1]\n# And now produce the uncertainty propagation weights\nreducer.set_params(uncertainty=True)\nblock_coords, velocity, uncertainty_weights = reducer.filter(coordinates,\n                                                             data.velocity_up,\n                                                             weights)\n\n\ndef setup_map(ax, title):\n    \"Draw coastlines, ticks, land, ocean, and set the title.\"\n    ax.set_title(title)\n    # Plot the land as a solid color\n    ax.add_feature(cfeature.LAND, edgecolor='black', facecolor='gray')\n    ax.add_feature(cfeature.OCEAN)\n    ax.set_extent(vd.get_region(coordinates), crs=crs)\n    # Set the proper ticks for a Cartopy map\n    ax.set_xticks(np.arange(-124, -115, 4), crs=crs)\n    ax.set_yticks(np.arange(33, 42, 2), crs=crs)\n    ax.xaxis.set_major_formatter(LongitudeFormatter())\n    ax.yaxis.set_major_formatter(LatitudeFormatter())\n\n\n# Now we can plot the different weights side by side on Mercator maps\nfig, axes = plt.subplots(1, 3, figsize=(13.5, 7),\n                         subplot_kw=dict(projection=ccrs.Mercator()))\ncrs = ccrs.PlateCarree()\ntitles = ['Variance weights', 'Uncertainty weights']\nweight_estimates = [variance_weights, uncertainty_weights]\nfor ax, title, w in zip(axes[:2], titles, weight_estimates):\n    setup_map(ax, title)\n    # Plot the original data locations\n    ax.plot(*coordinates, '.k', transform=crs, markersize=0.5)\n    # Plot the weights using a logarithmic color scale to highlight the\n    # differences\n    pc = ax.scatter(*block_coords, c=w, s=70, cmap='magma', transform=crs,\n                    norm=LogNorm())\n    plt.colorbar(pc, ax=ax, orientation='horizontal', pad=0.05)\n# Plot the original data uncertainties\nax = axes[2]\nsetup_map(ax, 'Data uncertainty')\n# Use a power law for the color scale because there is a lot of variability\n# Convert m/year to mm/year to have smaller values on the color bar\npc = ax.scatter(*coordinates, c=data.std_up*1000, s=10, transform=crs,\n                alpha=1, cmap='magma', norm=PowerNorm(gamma=1/2))\ncb = plt.colorbar(pc, ax=ax, orientation='horizontal', pad=0.05)\ncb.set_label('mm/yr')\nplt.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
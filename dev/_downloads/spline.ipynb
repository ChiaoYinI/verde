{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nGridding with splines and weights\n=================================\n\nBiharmonic spline interpolation is based on estimating vertical forces acting\non an elastic sheet that yield deformations in the sheet equal to the observed\ndata. The results are equivalent to using :class:`verde.ScipyGridder` with\n``method='cubic'`` but the interpolation is usually slower.\nThe advantage of using :class:`verde.Spline` is that you can assign weights to\nthe data to incorporate the data uncertainties or variance into the gridding.\n\nIn this example, we'll use :class:`verde.BlockMean` to decimate the data\nbecause it can calculate weights based on the data uncertainty from input data\nand pass it along to the spline.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom matplotlib.colors import PowerNorm\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\n# We need these two classes to set proper ticklabels for Cartopy maps\nfrom cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\nimport pyproj\nimport numpy as np\nimport verde as vd\n\n# We'll test this on the California vertical GPS velocity data because it comes\n# with the uncertainties\ndata = vd.datasets.fetch_california_gps()\ncoordinates = (data.longitude.values, data.latitude.values)\n\n# Use a Mercator projection for our Cartesian gridder\nprojection = pyproj.Proj(proj='merc', lat_ts=data.latitude.mean())\n\n# Now we can chain a block weighted mean and weighted spline together. We'll\n# use uncertainty propagation to calculate the new weights from block mean\n# because our data vary smoothly but have different uncertainties.\nspacing = 5/60   # 5 arc-minutes which we'll approximate to 5*111e3 meters\nchain = vd.Chain([\n    ('mean', vd.BlockMean(spacing=spacing*111e3, uncertainty=True)),\n    ('spline', vd.Spline(damping=1e-10))])\nprint(chain)\n\n# Split the data into a training and testing set. We'll use the training set to\n# grid the data and the testing set to validate our spline model. Weights need\n# to 1/uncertainty**2 for the error propagation in BlockMean to work.\ntrain, test = vd.train_test_split(projection(*coordinates), data.velocity_up,\n                                  weights=1/data.std_up**2, random_state=0)\n# Fit the model on the training set\nchain.fit(*train)\n# And calculate an R^2 score coefficient on the testing set. The best possible\n# score (perfect prediction) is 1. This can tell us how good our spline is at\n# predicting data that was not in the input dataset.\nscore = chain.score(*test)\nprint(\"\\nScore: {:.3f}\".format(score))\n\n# Create a grid of the vertical velocity and mask it to only show points close\n# to the actual data.\nregion = vd.get_region(coordinates)\nmask = vd.distance_mask((data.longitude, data.latitude), maxdist=0.5,\n                        region=region, spacing=spacing)\ngrid = chain.grid(region=region, spacing=spacing, projection=projection,\n                  dims=['latitude', 'longitude'], data_names=['velocity'])\ngrid = grid.where(mask)\n\n\ndef setup_map(ax):\n    \"Set the proper ticks for a Cartopy map and draw land and water\"\n    ax.set_xticks(np.arange(-124, -115, 4), crs=crs)\n    ax.set_yticks(np.arange(33, 42, 2), crs=crs)\n    ax.xaxis.set_major_formatter(LongitudeFormatter())\n    ax.yaxis.set_major_formatter(LatitudeFormatter())\n    ax.set_extent(region, crs=crs)\n    ax.add_feature(cfeature.LAND, facecolor='gray')\n    ax.add_feature(cfeature.OCEAN)\n\n\nfig, axes = plt.subplots(1, 2, figsize=(9, 7),\n                         subplot_kw=dict(projection=ccrs.Mercator()))\ncrs = ccrs.PlateCarree()\n# Plot the data uncertainties\nax = axes[0]\nax.set_title('Data uncertainty')\nsetup_map(ax)\n# Plot the uncertainties in mm/yr and using a power law for the color scale to\n# highlight the smaller values\npc = ax.scatter(*coordinates, c=data.std_up*1000, s=20, cmap='magma',\n                transform=crs, norm=PowerNorm(gamma=1/2))\ncb = plt.colorbar(pc, ax=ax, orientation='horizontal', pad=0.05)\ncb.set_label('uncertainty [mm/yr]')\n# Plot the gridded velocities\nax = axes[1]\nax.set_title('Spline interpolated vertical velocity')\nsetup_map(ax)\nmaxabs = np.max(np.abs([data.velocity_up.min(),\n                        data.velocity_up.max()]))*1000\npc = ax.pcolormesh(grid.longitude, grid.latitude, grid.velocity*1000,\n                   cmap='seismic', vmin=-maxabs, vmax=maxabs, transform=crs)\ncb = plt.colorbar(pc, ax=ax, orientation='horizontal', pad=0.05)\ncb.set_label('vertical velocity [mm/yr]')\nax.scatter(*coordinates, c='black', s=0.5, alpha=0.1, transform=crs)\nax.coastlines()\nplt.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}